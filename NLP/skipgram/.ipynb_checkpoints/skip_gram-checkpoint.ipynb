{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Skip Gram model\n",
    "Dataset: movie review 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz' <br>\n",
    "From this data set we will compute/fit the skipgram model of the Word2Vec Algorithm\n",
    "(https://arxiv.org/abs/1301.3781)\n",
    "<br>The Word2Vec Algorithm considers the word order.\n",
    "\n",
    "Skip-gram allows creation of word embeddings with word order informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "import io\n",
    "import tarfile\n",
    "import gzip\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.framework import ops\n",
    "import sys\n",
    "sys.path.insert(0, './utils')\n",
    "import text_helpers\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50         # How many sets of words to train on at once.\n",
    "embedding_size = 200    # The embedding size of each word to train.\n",
    "vocabulary_size = 10000 # How many words considered for training.\n",
    "generations = 100000    # How many iterations performed the training on.\n",
    "print_loss_every = 500  # Print the loss every so many iterations\n",
    "\n",
    "num_sampled = int(batch_size/2) # Number of negative examples to sample.\n",
    "window_size = 2         # How many words in skip-gram to consider left and right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare stop words\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "# Pick five test words. We are expecting synonyms to appear\n",
    "print_valid_every = 2000\n",
    "\n",
    "# set of words for testing\n",
    "valid_words = ['cliche', 'love', 'hate', 'silly', 'sad']\n",
    "# Later these will be transformed into indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, target = text_helpers.load_movie_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_helpers.normalize_text(texts, stops)\n",
    "\n",
    "# Texts must contain at least 3 words\n",
    "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > 2]\n",
    "texts = [x for x in texts if len(x.split()) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vocabulary\n",
    "Function that creates a word count dictionary, and assigns to 'RARE' the words not in the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1490, 28, 940, 205, 359]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build our data set and dictionaries\n",
    "word_dictionary = text_helpers.build_dictionary(texts, vocabulary_size)\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "text_data = text_helpers.text_to_numbers(texts, word_dictionary)\n",
    "\n",
    "# Get validation word keys\n",
    "valid_examples = [word_dictionary[x] for x in valid_words]\n",
    "valid_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of batch data\n",
    "The training dataset will consist in pair of words, where one word is the one at the center of the window, and the other one is one selected from the same window.\n",
    "\n",
    "Example: sentence \"the cat in the hat\"\n",
    "\n",
    "context word: [\"hat\"]\n",
    "target words: [\"the\", \"cat\", \"in\", \"the\"]\n",
    "context-target pairs: (\"hat\", \"the\"), (\"hat\", \"cat\"), (\"hat\", \"in\"), (\"hat\", \"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Embeddings:\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "# NCE loss parameters (explanation below)\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                               stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Create data/target placeholders\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "# Lookup the word embedding:\n",
    "# It will map the indices of the \n",
    "# words in the sentence to the one-hot-encoded vectors of our identity matrix.\n",
    "embed = tf.nn.embedding_lookup(embeddings, x_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "A softmax could be used, but since the target has 10000 categories, it is too sparse, causing difficulties in the convergence of a model. <br>\n",
    "\n",
    "To tackle this, the loss function called noise-contrastive error (NCE) can be used.<br> \n",
    "This \n",
    "NCE loss function turns our problem into a binary prediction, by predicting the word \n",
    "class versus random noise predictions. The num_sampled parameter is how much of \n",
    "the batch to turn into random noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loss from prediction\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=y_target,\n",
    "                                     inputs=embed,\n",
    "                                     num_sampled=num_sampled,\n",
    "                                     num_classes=vocabulary_size))\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "# Cosine similarity between words (to find nearby ones)\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "\n",
    "#Add variable initializer.\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_init = sess.run(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing\n",
    "Note to line *nearest = (-sim[j, :]).argsort()[1:top_k+1]* below: <br>\n",
    "The negative of the similarity matrix is used because argsort() sorts the values from least to greatest. In order to have the greatest numbers, sort in the opposite direction by taking the negative of the similarity matrix, then calling the argsort() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 500 : 77.35371398925781\n",
      "Loss at step 1000 : 49.351463317871094\n",
      "Loss at step 1500 : 49.78864288330078\n",
      "Loss at step 2000 : 31.838579177856445\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 2500 : 31.272642135620117\n",
      "Loss at step 3000 : 41.85002136230469\n",
      "Loss at step 3500 : 24.471689224243164\n",
      "Loss at step 4000 : 15.03326416015625\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 4500 : 12.656588554382324\n",
      "Loss at step 5000 : 22.972822189331055\n",
      "Loss at step 5500 : 28.042152404785156\n",
      "Loss at step 6000 : 19.314712524414062\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 6500 : 10.55569839477539\n",
      "Loss at step 7000 : 5.275065898895264\n",
      "Loss at step 7500 : 4.601869106292725\n",
      "Loss at step 8000 : 6.647920608520508\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 8500 : 3.8728890419006348\n",
      "Loss at step 9000 : 5.116858005523682\n",
      "Loss at step 9500 : 4.090730667114258\n",
      "Loss at step 10000 : 4.653043746948242\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 10500 : 4.118194580078125\n",
      "Loss at step 11000 : 13.59912109375\n",
      "Loss at step 11500 : 14.047861099243164\n",
      "Loss at step 12000 : 8.500486373901367\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 12500 : 3.0166678428649902\n",
      "Loss at step 13000 : 2.164107084274292\n",
      "Loss at step 13500 : 3.105039119720459\n",
      "Loss at step 14000 : 3.2326061725616455\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 14500 : 2.2710297107696533\n",
      "Loss at step 15000 : 2.3691000938415527\n",
      "Loss at step 15500 : 1.6509944200515747\n",
      "Loss at step 16000 : 1.9098501205444336\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 16500 : 6.4838690757751465\n",
      "Loss at step 17000 : 2.376873254776001\n",
      "Loss at step 17500 : 4.113985061645508\n",
      "Loss at step 18000 : 2.2069895267486572\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 18500 : 2.6767067909240723\n",
      "Loss at step 19000 : 1.8699661493301392\n",
      "Loss at step 19500 : 1.0265053510665894\n",
      "Loss at step 20000 : 4.759484767913818\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 20500 : 1.851930856704712\n",
      "Loss at step 21000 : 1.3072465658187866\n",
      "Loss at step 21500 : 1.9663854837417603\n",
      "Loss at step 22000 : 1.141076683998108\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 22500 : 2.7179338932037354\n",
      "Loss at step 23000 : 2.748880624771118\n",
      "Loss at step 23500 : 2.22475528717041\n",
      "Loss at step 24000 : 1.6857397556304932\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 24500 : 0.5911130309104919\n",
      "Loss at step 25000 : 0.6691019535064697\n",
      "Loss at step 25500 : 0.990801215171814\n",
      "Loss at step 26000 : 1.4723190069198608\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 26500 : 1.3032909631729126\n",
      "Loss at step 27000 : 1.7597174644470215\n",
      "Loss at step 27500 : 3.5595827102661133\n",
      "Loss at step 28000 : 1.2383350133895874\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 28500 : 1.7726408243179321\n",
      "Loss at step 29000 : 1.104441523551941\n",
      "Loss at step 29500 : 1.2813671827316284\n",
      "Loss at step 30000 : 0.9204170107841492\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 30500 : 1.0151830911636353\n",
      "Loss at step 31000 : 1.2895829677581787\n",
      "Loss at step 31500 : 1.8538410663604736\n",
      "Loss at step 32000 : 2.1409292221069336\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 32500 : 2.65484619140625\n",
      "Loss at step 33000 : 0.523542046546936\n",
      "Loss at step 33500 : 1.4209411144256592\n",
      "Loss at step 34000 : 1.289392113685608\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 34500 : 2.0144715309143066\n",
      "Loss at step 35000 : 0.975709080696106\n",
      "Loss at step 35500 : 0.8755348324775696\n",
      "Loss at step 36000 : 1.0762921571731567\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 36500 : 1.0897375345230103\n",
      "Loss at step 37000 : 2.9010508060455322\n",
      "Loss at step 37500 : 1.5280669927597046\n",
      "Loss at step 38000 : 0.8578202724456787\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 38500 : 3.251991033554077\n",
      "Loss at step 39000 : 1.2583551406860352\n",
      "Loss at step 39500 : 1.2675714492797852\n",
      "Loss at step 40000 : 1.3330646753311157\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 40500 : 1.1049021482467651\n",
      "Loss at step 41000 : 0.8236991763114929\n",
      "Loss at step 41500 : 1.040757179260254\n",
      "Loss at step 42000 : 1.1180492639541626\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 42500 : 1.111670970916748\n",
      "Loss at step 43000 : 0.9697213172912598\n",
      "Loss at step 43500 : 0.810469388961792\n",
      "Loss at step 44000 : 1.449158787727356\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 44500 : 5.951934337615967\n",
      "Loss at step 45000 : 7.212289810180664\n",
      "Loss at step 45500 : 2.6000659465789795\n",
      "Loss at step 46000 : 1.0662726163864136\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 46500 : 0.7866085767745972\n",
      "Loss at step 47000 : 0.8659884929656982\n",
      "Loss at step 47500 : 0.41827449202537537\n",
      "Loss at step 48000 : 3.4145238399505615\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 48500 : 1.2599033117294312\n",
      "Loss at step 49000 : 0.7146939039230347\n",
      "Loss at step 49500 : 1.0825419425964355\n",
      "Loss at step 50000 : 0.9862744212150574\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 50500 : 0.726459264755249\n",
      "Loss at step 51000 : 1.352489948272705\n",
      "Loss at step 51500 : 0.4515782296657562\n",
      "Loss at step 52000 : 1.211063027381897\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 52500 : 1.1135809421539307\n",
      "Loss at step 53000 : 1.7491286993026733\n",
      "Loss at step 53500 : 1.4014859199523926\n",
      "Loss at step 54000 : 1.5135715007781982\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 54500 : 0.9777427911758423\n",
      "Loss at step 55000 : 1.1595184803009033\n",
      "Loss at step 55500 : 0.7938528656959534\n",
      "Loss at step 56000 : 1.9719756841659546\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 56500 : 0.9950860738754272\n",
      "Loss at step 57000 : 1.0614368915557861\n",
      "Loss at step 57500 : 1.1281847953796387\n",
      "Loss at step 58000 : 2.5364866256713867\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 58500 : 0.9189188480377197\n",
      "Loss at step 59000 : 1.1447588205337524\n",
      "Loss at step 59500 : 1.3331663608551025\n",
      "Loss at step 60000 : 1.063379168510437\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 60500 : 1.0370556116104126\n",
      "Loss at step 61000 : 5.915192127227783\n",
      "Loss at step 61500 : 0.8578831553459167\n",
      "Loss at step 62000 : 1.0981420278549194\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 62500 : 1.0710524320602417\n",
      "Loss at step 63000 : 1.1719380617141724\n",
      "Loss at step 63500 : 0.9891871809959412\n",
      "Loss at step 64000 : 0.9900640845298767\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 64500 : 1.2256015539169312\n",
      "Loss at step 65000 : 1.1542490720748901\n",
      "Loss at step 65500 : 0.9921606183052063\n",
      "Loss at step 66000 : 1.1166740655899048\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 66500 : 0.9543309807777405\n",
      "Loss at step 67000 : 0.7782095074653625\n",
      "Loss at step 67500 : 1.2036532163619995\n",
      "Loss at step 68000 : 0.9879474639892578\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 68500 : 0.9984826445579529\n",
      "Loss at step 69000 : 1.1128332614898682\n",
      "Loss at step 69500 : 1.1723692417144775\n",
      "Loss at step 70000 : 1.120856761932373\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 70500 : 1.0633325576782227\n",
      "Loss at step 71000 : 0.8695576190948486\n",
      "Loss at step 71500 : 1.0714678764343262\n",
      "Loss at step 72000 : 1.2766692638397217\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 72500 : 1.1675937175750732\n",
      "Loss at step 73000 : 0.8924819231033325\n",
      "Loss at step 73500 : 1.0922127962112427\n",
      "Loss at step 74000 : 0.9601470828056335\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 74500 : 3.1290388107299805\n",
      "Loss at step 75000 : 1.1743953227996826\n",
      "Loss at step 75500 : 0.9645028710365295\n",
      "Loss at step 76000 : 0.24405761063098907\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 76500 : 1.4617234468460083\n",
      "Loss at step 77000 : 1.3120934963226318\n",
      "Loss at step 77500 : 1.2447913885116577\n",
      "Loss at step 78000 : 1.0928561687469482\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 78500 : 1.0579500198364258\n",
      "Loss at step 79000 : 0.5445477962493896\n",
      "Loss at step 79500 : 1.3206411600112915\n",
      "Loss at step 80000 : 1.587292194366455\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 80500 : 1.1545634269714355\n",
      "Loss at step 81000 : 1.0486563444137573\n",
      "Loss at step 81500 : 0.9459181427955627\n",
      "Loss at step 82000 : 1.224900245666504\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 82500 : 1.1660743951797485\n",
      "Loss at step 83000 : 0.7901732921600342\n",
      "Loss at step 83500 : 1.1739988327026367\n",
      "Loss at step 84000 : 1.1019575595855713\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 84500 : 1.0544044971466064\n",
      "Loss at step 85000 : 1.0878009796142578\n",
      "Loss at step 85500 : 0.8492382168769836\n",
      "Loss at step 86000 : 1.1970893144607544\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 86500 : 1.2020955085754395\n",
      "Loss at step 87000 : 1.0077210664749146\n",
      "Loss at step 87500 : 1.1618969440460205\n",
      "Loss at step 88000 : 1.0445178747177124\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 88500 : 1.1792209148406982\n",
      "Loss at step 89000 : 1.0590742826461792\n",
      "Loss at step 89500 : 0.7170229554176331\n",
      "Loss at step 90000 : 1.168634295463562\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 90500 : 1.184501051902771\n",
      "Loss at step 91000 : 0.9567770957946777\n",
      "Loss at step 91500 : 0.7719346880912781\n",
      "Loss at step 92000 : 0.8836759924888611\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 92500 : 1.3553986549377441\n",
      "Loss at step 93000 : 1.3205082416534424\n",
      "Loss at step 93500 : 1.2525455951690674\n",
      "Loss at step 94000 : 0.9033265113830566\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 94500 : 0.7654021978378296\n",
      "Loss at step 95000 : 1.4337283372879028\n",
      "Loss at step 95500 : 0.9518629312515259\n",
      "Loss at step 96000 : 1.1099220514297485\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 96500 : 0.9710115790367126\n",
      "Loss at step 97000 : 1.703837275505066\n",
      "Loss at step 97500 : 1.320348858833313\n",
      "Loss at step 98000 : 0.9931538105010986\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n",
      "Loss at step 98500 : 1.11275315284729\n",
      "Loss at step 99000 : 1.1215596199035645\n",
      "Loss at step 99500 : 0.8349418640136719\n",
      "Loss at step 100000 : 2.208498001098633\n",
      "Nearest to cliche: confident, style, oleander, drown, fuller,\n",
      "Nearest to love: civic, rocky, ivan, miyazaki, burns,\n",
      "Nearest to hate: whiff, gravity, scientific, smiles, resultado,\n",
      "Nearest to silly: mug, koreas, kissing, lots, toback,\n",
      "Nearest to sad: smooth, suggestive, selfimportance, steven, disjointed,\n"
     ]
    }
   ],
   "source": [
    "# Run the skip gram model.\n",
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = text_helpers.generate_batch_data(text_data, batch_size, window_size)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    # Run the train step\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "\n",
    "    # Return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print(\"Loss at step {} : {}\".format(i+1, loss_val))\n",
    "      \n",
    "    # Validation: Print some random words and top 5 related words\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                score = sim[j,nearest[k]]\n",
    "                log_str = \"%s %s,\" % (log_str, close_word)\n",
    "            print(log_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the closest words to the validation ones are not synonyms. <br>\n",
    "This is because synonyms very rarely do actually appear next to each other \n",
    "in sentences. The model predicts which words are in proximity to each \n",
    "other in our data set. We hope that using an embedding like this would make prediction easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
